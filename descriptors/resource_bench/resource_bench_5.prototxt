layer {
    input_height: 30
    input_width: 30
    num_inputs: 32
    output_height: 24
    output_width: 24
    num_outputs: 96

    conv: {
        activation: Relu
        kernel_size: 7

        // Only this 3 lines should be changed!
        // 1 <= worker_factor <= num_inputs
        // 1 <= conv_folding_factor <= num_outputs
        // 1 <= kernel_folding_factor <= kernel_size * kernel_size
        worker_factor: 1
        conv_folding_factor: 5
        kernel_folding_factor: 25
    }
}
