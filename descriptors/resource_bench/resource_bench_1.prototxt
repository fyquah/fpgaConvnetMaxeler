layer {
    input_height: 30
    input_width: 30
    num_inputs: 32
    output_height: 26
    output_width: 26
    num_outputs: 96

    conv: {
        activation: Relu
        kernel_size: 5

        worker_factor: 16
        conv_folding_factor: 6
        kernel_folding_factor: 5
    }
}
