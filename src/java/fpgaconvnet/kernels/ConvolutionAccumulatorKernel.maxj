package fpgaconvnet.kernels;

import java.lang.RuntimeException;

import java.util.List;
import java.util.ArrayList;

import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count.Counter;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.CounterChain;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Stream.OffsetExpr;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.memory.Memory;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEVar;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVector;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVectorType;
import com.maxeler.maxcompiler.v2.kernelcompiler.KernelParameters;

import fpgaconvnet.IterationCounter;
import fpgaconvnet.GlobalConfig;
import fpgaconvnet.Utils;
import fpgaconvnet.protos.Parameters.LayerParameter;
import fpgaconvnet.protos.Parameters.ConvolutionParameter;


public class ConvolutionAccumulatorKernel extends ConvolutionKernelBase {

    private final List<Memory<DFEVar>> biasRoms;
    private final LayerParameter layerParams;
    private final ConvolutionParameter convParams;
    private final DFEVectorType<DFEVar> inputVectorType;
    private final DFEVectorType<DFEVar> outputVectorType;
    private final IterationCounter ctr;

    private List<DFEVar> map(List<DFEVector<DFEVar>> list, int index) {
        /* Gets the index-th element of every vector in list. */
        List<DFEVar> ret = new ArrayList<DFEVar>();
        for (DFEVector<DFEVar> vector: list) {
            ret.add(vector[index]);
        }
        return ret;
    }

    private DFEVar relu(DFEVar x) {
        return (x > 0 ? x : 0);
    }

    private DFEVar simpleCounter(int x) {
        return control.count.simpleCounter(Utils.ceilLog2(x), x);
    }

    private DFEVar getChannelIter(DFEVar lookAhead, DFEVar weightUsageIter, DFEVar kernelIter) {
        /* Number of iterations to spend per channel. */
        final int channelIterations = ctr.getConvolutionIterations() * ctr.getKernelIterations();
        final int weightsUsageCycles = ctr.getWeightIterations();

        DFEVar channelIterBase = control.count.makeCounter(
                control.count.makeParams(Utils.ceilLog2(channelIterations))
                .withInitValue(0l)
                .withInc(weightsUsageCycles * ctr.getKernelIterations())
                .withMax(channelIterations)
                .withEnable(lookAhead.eq(layerParams.getConv().getLookAhead() - 1)
                            & weightUsageIter.eq(weightsUsageCycles - 1)
                            & kernelIter.eq(ctr.getKernelIterations() - 1))
            ).getCount();

        /* Defines the stage at which we are using a particular channel. */
        DFEVar channelIter = dfeUInt(Utils.ceilLog2(channelIterations)).newInstance(this);
        optimization.pushPipeliningFactor(0.0);
        channelIter <== (weightUsageIter.eq(0) & kernelIter.eq(0)
                         ? channelIterBase
                         : Utils.modInc(stream.offset(channelIter, -1), channelIterations));
        optimization.popPipeliningFactor();

        return channelIter;
    }

    private DFEVar getConvIter(DFEVar lookAheadIter, DFEVar weightUsageIter, DFEVar kernelIter) {

        /* The number of convIter that a weightIter yields. */
        DFEVar convIterBase = control.count.makeCounter(
                control.count.makeParams(Utils.ceilLog2(getConvolutionIterations()))
                .withInitValue(0)
                .withInc(ctr.getWeightIterations())
                .withMax(getConvolutionIterations())
                .withEnable(lookAheadIter.eq(layerParams.getConv().getLookAhead() - 1)
                            & weightUsageIter.eq(ctr.getWeightIterations() - 1)
                            & kernelIter.eq(ctr.getKernelIterations() - 1))
            ).getCount();

        DFEVar convIter = dfeUInt(
                Utils.ceilLog2(getConvolutionIterations())).newInstance(this);
        optimization.pushPipeliningFactor(0.0);
        convIter <== (kernelIter.eq(0)
                      ? (weightUsageIter.eq(0)
                         ? convIterBase
                         : Utils.modInc(stream.offset(convIter, -1), getConvolutionIterations()))
                      : stream.offset(convIter, -1));
        optimization.popPipeliningFactor();

        return convIter;
    }

    public ConvolutionAccumulatorKernel(KernelParameters kp, LayerParameter params) {
        super(kp, params);

        ctr = new IterationCounter(params);
        layerParams = params;
        convParams = params.getConv();
        inputVectorType = new DFEVectorType<DFEVar>(
                GlobalConfig.dataType, layerParams.getConv().getConvFoldingFactor());
        outputVectorType = new DFEVectorType<DFEVar>(
                GlobalConfig.dataType, layerParams.getNumOutputs());
        biasRoms = new ArrayList<Memory<DFEVar>>();

        for (int i = 0; i < params.getNumOutputs() ; i++) {
            biasRoms.add(mem.alloc(GlobalConfig.dataType, 2));
        }

        DFEVector<DFEVar> output = outputVectorType.newInstance(this);
        List<DFEVector<DFEVar>> inputVectors = new ArrayList<DFEVector<DFEVar>>();

        // Counters for initialization
        DFEVar initChannel = control.count.simpleCounter(
                Utils.ceilLog2(initCycles()),
                initCycles());
        Counter initializationCompleteCtr = control.count.makeCounter(
                control.count.makeParams(1)
                .withInc(1)
                .withMax(1)
                .withWrapMode(Count.WrapMode.STOP_AT_MAX)
                .withEnable(initChannel.eq(initCycles() - 1)));

        DFEVar initWeightsFlag = io.scalarInput("init", dfeUInt(1));
        DFEVar initializationComplete = initializationCompleteCtr.getCount();
        DFEVar isInitializingWeights = initWeightsFlag & ~initializationComplete;

        // Runtime Counters and loop variables
        DFEVar convIter;
        DFEVar lookAheadIter;
        DFEVar channelIter;
        DFEVar outputControlFlag;

        if (convParams.getLookAhead() == 1) {
            final int totalIter =
                    getKernelIterations() * getConvolutionIterations() * getSchedulerIterations();
            CounterChain chain = control.count.makeCounterChain(~isInitializingWeights);
            DFEVar schedulerIter = Utils.chainCounterOrZero(this, chain, getSchedulerIterations());
            convIter = Utils.chainCounterOrZero(this, chain, getConvolutionIterations());
            Utils.chainCounterOrZero(this, chain, getKernelIterations());
            channelIter = control.count.simpleCounter(Utils.ceilLog2(totalIter), totalIter);

            lookAheadIter = constant.var(dfeBool(), 0);
            outputControlFlag = ~isInitializingWeights & channelIter.eq(totalIter - 1);

        } else {
            CounterChain chain = control.count.makeCounterChain(~isInitializingWeights);
            DFEVar loopIter = Utils.chainCounterOrZero(this, chain, ctr.getRevisitIterations());
            lookAheadIter = Utils.chainCounterOrZero(this, chain, convParams.getLookAhead());
            DFEVar weightIter = Utils.chainCounterOrZero(this, chain, ctr.getWeightIterations());
            DFEVar kernelIter = simpleCounter(getKernelIterations());

            convIter = getConvIter(lookAheadIter, weightIter, kernelIter);
            channelIter = getChannelIter(lookAheadIter, weightIter, kernelIter);

            /* This is wrong - wouldn't work is the bram_factor numbers don't divide the
             * n_scheduler * n_convolution nicely.
             */
            outputControlFlag =
                    ~isInitializingWeights
                    & loopIter.eq(ctr.getRevisitIterations() - 1)
                    & weightIter.eq(ctr.getWeightIterations() - 1);
        }

        // Kernel IO interfaces
        DFEVar rawBiasInput = io.input(getBiasInputName(),
                                       GlobalConfig.cpuType,
                                       isInitializingWeights);
        DFEVar biasInput = rawBiasInput.cast(GlobalConfig.dataType);
        DFEVar inputControlFlag = ~isInitializingWeights;

        for (int i = 0 ; i < layerParams.getConv().getWorkerFactor() ; i++) {
            inputVectors.add(io.input(getInputName(i), inputVectorType, inputControlFlag));
        }
        if (layerParams.getIsLastLayer()) {
            /* Last layer - cast from fixed point to dfe float. */
            DFEVectorType<DFEVar> rawOutputVectorType = new DFEVectorType<DFEVar>(
                    GlobalConfig.cpuType, layerParams.getNumOutputs());
            DFEVector<DFEVar> rawOutput = rawOutputVectorType.newInstance(this);

            for (int i = 0 ; i < layerParams.getNumOutputs() ; i++) {
                rawOutput[i] <== output[i].cast(GlobalConfig.cpuType);
            }
            io.output(getOutputName(), rawOutput, rawOutputVectorType, outputControlFlag);

        } else {
            io.output(getOutputName(), output, outputVectorType, outputControlFlag);
        }

        for (int i = 0 ; i < params.getNumOutputs() ; i++) {
            biasRoms[i].write(
                    constant.var(dfeUInt(1), 0),
                    biasInput,
                    isInitializingWeights & (initChannel.eq(i)));
        }

        List<DFEVector<DFEVar>> lookAheadOutputs = new ArrayList<DFEVector<DFEVar>>();

        // logic for accumulating outputs
        for (int lookAhead = 0; lookAhead < convParams.getLookAhead() ; lookAhead++) {
            DFEVector<DFEVar> lookAheadOutput = outputVectorType.newInstance(this);

            for (int channel = 0 ; channel < layerParams.getNumOutputs() ; channel++) {

                final int convFoldingFactor = layerParams.getConv().getConvFoldingFactor();
                DFEVar acc = GlobalConfig.dataType.newInstance(this);
                DFEVar initFlag =
                        lookAheadIter.eq(lookAhead)
                        & channelIter.eq(
                            (channel / layerParams.getConv().getConvFoldingFactor()));
                DFEVar sumVars = Utils.treeReduceAdd(map(inputVectors, channel % convFoldingFactor));
                DFEVar channelOutputFlag = (
                        lookAheadIter.eq(lookAhead)
                        & convIter.eq(channel / layerParams.getConv().getConvFoldingFactor()));
                DFEVar muxedSumVars = channelOutputFlag ? sumVars : 0.0;
                DFEVar bias = biasRoms[channel].read(constant.var(dfeUInt(1), 0));
                DFEVar carriedSum = stream.offset(acc, -1);

                // push and pop pipelining factor required to make sure this accumulator runs at
                // 0-tick latency (i.e: stream.offset(-1) works)
                optimization.pushPipeliningFactor(0);
                DFEVar prevAcc = initFlag ? bias : carriedSum;
                optimization.popPipeliningFactor();

                acc <== muxedSumVars + prevAcc;
                lookAheadOutput[channel] <== acc;
            }

            lookAheadOutputs.add(lookAheadOutput);
        }

        DFEVector<DFEVar> chosen = control.mux(lookAheadIter, lookAheadOutputs);

        if (layerParams.getActivation() == LayerParameter.Activation.None) {
            output <== chosen;

        } else if (layerParams.getActivation() == LayerParameter.Activation.Relu) {
            for (int i = 0 ; i < layerParams.getNumOutputs() ; i++) {
                output[i] <== relu(chosen[i]);
            }

        } else {
            throw new RuntimeException("Unsupported activation type");
        }

        // debugging output
        // String s_output = "[";
        // String s_conv = "[\n";
        // List<Object> debugValues = new ArrayList<Object>();
        // debugValues.add((Integer) layerParams.getLayerId());
        // debugValues.add(outputControlFlag);
        // debugValues.add(pipelineStage);
        // debugValues.add(pipelineLengthVar);
        // debugValues.add(schedulerIter);
        // debugValues.add(convIter);
        // debugValues.add(kernelIter);
        // for (DFEVector<DFEVar> vector: inputVectors) {
        //     s_conv += "[";
        //     for (int i = 0 ; i < params.getConv().getConvFoldingFactor() ; i++) {
        //         debugValues.add(vector[i]);
        //         s_conv += "%.3f, ";
        //     }
        //     s_conv += "]\n";
        // }
        // for (int i = 0 ; i < params.getNumOutputs() ; i++) {
        //     debugValues.add(output[i]);
        //     s_output += "%.3f, ";
        // }
        // s_conv += "]";
        // s_output += "]";
        // debug.simPrintf(
        //         "ConvolutionAccumulatorKernel[layer = %d]"
        //         + " outputControlFlag = %d"
        //         + " pipelineStage = %d pipelineLengthVar = %d"
        //         + " schedulerIter = %d convIter = %d kernelIter = %d"
        //         + "\ninputs = " + s_conv + "\n"
        //         + "\noutput = " + s_output + "\n",
        //         debugValues.toArray(new Object [1]));
    }

    public String getBiasInputName() {
        return "input_bias";
    }

    public String getInputName(int workerId) {
        return "input_" + workerId;
    }

    public String getOutputName() {
        return "output";
    }

    public String getOffsetExprName() {
        return String.format("offset");
    }

    public int initCycles() {
        // maxeler requires streams to be a multiple of 16 bytes = 4 * 4 bytes.
        return Utils.divCeil(layerParams.getNumOutputs(), 4) * 4;
    }

    public int cyclesPerImage() {
        int totalOutputPixels = layerParams.getOutputHeight() * layerParams.getOutputWidth();
        int cyclesPerPixel = getSchedulerIterations()
                * getConvolutionIterations()
                * getKernelIterations();
        return totalOutputPixels * cyclesPerPixel;
    }
}
