/* Most of the logic of resource alloc should go here. */

package fpgaconvnet;

import java.lang.RuntimeException;

import java.util.ArrayList;

import fpgaconvnet.protos.Parameters;
import fpgaconvnet.protos.Parameters.LayerParameter;
import fpgaconvnet.protos.Parameters.ConvolutionParameter;
import fpgaconvnet.protos.Parameters.PoolingParameter;
import fpgaconvnet.protos.Parameters.LrnParameter;


public class ParametersTransformer {
    private Parameters.Network networkParameters;

    public ParametersTransformer(Parameters.Network networkParametersIn) {
        networkParameters = networkParametersIn;
    }

    private void raiseError(int layer_id, String message) throws RuntimeException {
        throw new RuntimeException(String.format("Invalid layer spec at layer %d => %s",
                                         layer_id, message));
    }

    private void raiseWarning(int layer_id, String message) {
        System.out.println(String.format("[WARNING layer %d] => %s", layer_id, message));
    }

    private LayerParameter parseLayer(int layer_id, LayerParameter layerParameter,
                                      LayerParameter previousLayer, int weightAddressBase) {
        LayerParameter.Builder builder = LayerParameter.newBuilder();
        int inputHeight = 0;
        int inputWidth = 0;
        int numInputs = 0;
        int numOutputs = 0;
        builder.mergeFrom(layerParameter);

        /* Generic parameters (layer-type independent)
         * - inputHeight
         * - inputWidth
         * - numInputs
         * Layer-Dependent parameters
         * - outputHeight
         * - outputWidth
         * - numOutputs
         */
        if (layer_id == 0) {
            if (!layerParameter.hasInputHeight()
                    || !layerParameter.hasInputWidth()
                    || !layerParameter.hasNumInputs()) {
                raiseError(layer_id,
                           "First layer must specify the input height, width and num_inputs ("
                           + "i.e: The number of channels)");
            }
            inputHeight = layerParameter.getInputHeight();
            inputWidth = layerParameter.getInputWidth();
            numInputs = layerParameter.getNumInputs();
        } else {
            if (layer_id - 1 != previousLayer.getLayerId()) {
                raiseError(layer_id, "previousLayer.getLayerId() != layer_id - 1.");
            }
            if (layerParameter.hasInputHeight()
                    && previousLayer.getOutputHeight() != layerParameter.getInputHeight()) {
                raiseError(layer_id,
                           String.format("Input height mismatch - expecting %d but got %d instead",
                                         previousLayer.getOutputHeight(),
                                         layerParameter.getInputHeight()));
            }
            if (layerParameter.hasInputWidth()
                    && previousLayer.getOutputWidth() != layerParameter.getInputWidth()) {
                raiseError(layer_id,
                           String.format("Input Width mismatch - expecting %d but got %d instead",
                                         previousLayer.getOutputWidth(),
                                         layerParameter.getInputWidth()));
            }
            if (layerParameter.hasNumInputs()
                    && previousLayer.getNumOutputs() != layerParameter.getNumInputs()) {
                raiseError(layer_id,
                           String.format("Num Inputs mismatch - expecting %d but got %d instead",
                                         previousLayer.getNumOutputs(),
                                         layerParameter.getNumInputs()));
            }

            inputHeight = previousLayer.getOutputHeight();
            inputWidth = previousLayer.getOutputWidth();
            numInputs = previousLayer.getNumOutputs();
        }

        /* Calculate the new dimensions based on the data that we have on the ops. */
        LayerParameter.ParamsCase paramsCase = layerParameter.getParamsCase();
        int stride = -1;
        int pad = -1;
        int kernelSize = -1;

        if (paramsCase.equals(LayerParameter.ParamsCase.CONV)) {
            ConvolutionParameter convParams = layerParameter.getConv();
            int bramFactor = layerParameter.getConv().getBramFactor();

            if (!convParams.hasKernelSize() || convParams.getKernelSize() % 2 == 0) {
                raiseError(layer_id, ("Missing /invalid conv kernel_size. Conv's Kernel size "
                                      + "has to be an odd number."));
            }
            if (!layerParameter.hasNumOutputs() || layerParameter.getNumOutputs() == 0) {
                raiseError(layer_id, "numOutputs must be a positive integer in conv layers!");
            }
            if (!layerParameter.getConv().hasBramFactor()) {
                bramFactor =
                        convParams.getWorkerFactor()
                        * convParams.getConvFoldingFactor()
                        * Utils.divCeil(layerParameter.getNumInputs(),
                                        convParams.getWorkerFactor())
                        * Utils.divCeil(layerParameter.getNumOutputs(),
                                        convParams.getConvFoldingFactor());
            }

            stride = convParams.getStride();
            pad = convParams.getPad();
            stride = convParams.hasStride() ? convParams.getStride() : 1;
            kernelSize = convParams.getKernelSize();
            numOutputs = layerParameter.getNumOutputs();

            builder.setConv(
                    ConvolutionParameter.newBuilder()
                    .mergeFrom(convParams)
                    .setStride(stride)
                    .setBramFactor(bramFactor)
                    .setWeightAddressBase(weightAddressBase)
                    .build());

        } else if (paramsCase.equals(LayerParameter.ParamsCase.POOL)) {
            PoolingParameter poolParams = layerParameter.getPool();

            if (layerParameter.hasNumOutputs() &&
                    layerParameter.getNumOutputs() != numInputs) {
                    raiseError(layer_id,
                               "num_inputs and num_outputs for pooling layer should be the same");
            }

            pad = 0;
            kernelSize = poolParams.getDim();
            stride = poolParams.hasStride() ? poolParams.getStride() : kernelSize;
            numOutputs = numInputs;

            builder.setPool(
                    PoolingParameter.newBuilder()
                    .mergeFrom(poolParams)
                    .setStride(stride)
                    .build());

        } else if (paramsCase.equals(LayerParameter.ParamsCase.LRN)) {
            LrnParameter lrnParams = layerParameter.getLrn();

            numOutputs = numInputs;
            pad = 0;
            kernelSize = 1;
            stride = 1;

            if (layerParameter.hasNumOutputs() &&
                    layerParameter.getNumOutputs() != numInputs) {
                    raiseError(layer_id,
                               "num_inputs and num_outputs for LRN layer should be the same");
            }

            builder.setLrn(lrnParams);

        } else {
            raiseError(layer_id, "Either pool of conv has to be set!");

        }

        final int unstridedHeight = inputHeight - kernelSize + 2 * pad;
        final int unstridedWidth = inputWidth - kernelSize + 2 * pad;
        int expectedHeight;
        int expectedWidth;


        if (paramsCase.equals(LayerParameter.ParamsCase.CONV)) {
            expectedHeight = unstridedHeight / stride + 1;
            expectedWidth = unstridedWidth / stride + 1;

        } else {
            expectedHeight = Utils.divCeil(unstridedHeight, stride) + 1;
            expectedWidth = Utils.divCeil(unstridedWidth, stride) + 1;

            if (unstridedHeight % stride != 0) {
                raiseWarning(layer_id,
                             ("pool layer `stride` doesn't divide "
                              + "`input_height - kernel_size + 2 * pad`!"
                              + ". Border cases may be approximated"));
            }
            if (unstridedWidth % stride != 0) {
                raiseWarning(layer_id,
                             ("pool layer `stride` doesn't divide "
                              + "`input_width - kernel_size + 2 * pad`!"
                              + ". Border cases may be approximated!"));
            }

        }


        builder.setInputHeight(inputHeight);
        builder.setInputWidth(inputWidth);
        builder.setNumInputs(numInputs);
        builder.setOutputHeight(expectedHeight);
        builder.setOutputWidth(expectedWidth);
        builder.setNumOutputs(numOutputs);
        builder.setLayerId(layer_id);
        if (networkParameters.getLayerCount() - 1 == layer_id) {
            builder.setIsLastLayer(true);
        }
        if (layer_id == 0) {
            builder.setIsFirstLayer(true);
        }
        return builder.build();
    }

    private boolean checkOutOfRange(int x, int max) {
        return x < 1 || x > max;
    }

    public void verifyLayerParameters(LayerParameter layer) {
        LayerParameter.ParamsCase paramsCase = layer.getParamsCase();

        switch (paramsCase) {
        case CONV:
            int totalKernelSize = layer.getConv().getKernelSize() * layer.getConv().getKernelSize();

            if (checkOutOfRange(layer.getConv().getKernelFoldingFactor(), totalKernelSize)) {
                raiseError(layer.getLayerId(),
                        "kernel folding factor in convolution layer must be strictly between "
                        + "inclusive 1 and kernel_size * kernel_size");
            }
            if (checkOutOfRange(layer.getConv().getWorkerFactor(), layer.getNumInputs())) {
                raiseError(layer.getLayerId(),
                        "Worker factor in convolution layer must be strictly between "
                        + "inclusive 1 and num_inputs");
            }
            if (layer.getNumInputs() % layer.getConv().getWorkerFactor() != 0) {
                raiseError(layer.getLayerId(),
                        "worker factor must divide num inputs. This is required to optimize"
                        + " fifo usage.");
            }
            if (checkOutOfRange(layer.getConv().getConvFoldingFactor(), layer.getNumOutputs())) {
                raiseError(layer.getLayerId(),
                        "Conv folding factor in convolution layer must be strictly between "
                        + "inclusive 1 and num_outputs");
            }

            IterationCounter ctr = new IterationCounter(layer);
            int bramPerConvolver = layer.getConv().getBramFactor()
                    / (layer.getConv().getWorkerFactor() * layer.getConv().getConvFoldingFactor());

            if (bramPerConvolver == 0 || layer.getConv().getBramFactor() % bramPerConvolver != 0) {
                raiseError(layer.getLayerId(),
                        "bram_factor in convolution layer must be a multiple of"
                        + " (worker_factor * conv_folding_factor)");
            }
            boolean flag =
                    (bramPerConvolver < ctr.getConvolutionIterations()
                        && ctr.getConvolutionIterations() % bramPerConvolver != 0)
                    || (bramPerConvolver > ctr.getConvolutionIterations()
                        && bramPerConvolver % ctr.getConvolutionIterations() != 0);

            if (flag) {
                raiseError(layer.getLayerId(),
                        "bram_factor / (worker_factor * conv_folding_factor) in convolution "
                        + " layer must be a factor or a multiple of"
                        + " ceil(num_outputs / conv_folding_factor)");
            }

            int totalPixels = layer.getOutputHeight() * layer.getOutputWidth();
            if (layer.getConv().getLookAhead() < 1
                    || totalPixels % layer.getConv().getLookAhead() != 0
                    || layer.getConv().getLookAhead() > totalPixels) {
                raiseError(layer.getLayerId(),
                        "lookAhead must be > 1, a factor of total OUTPUT pixels and must be <="
                        + " the total number of pixels");
            }
            if (bramPerConvolver >= ctr.getSchedulerIterations() * ctr.getConvolutionIterations()
                    && layer.getConv().getLookAhead() != 1) {
                raiseError(layer.getLayerId(),
                        "lookAhead must be 1, when entier layer weights are stored on the"
                        + " on-chip memory");

            }
            break;

        case POOL:
            if (layer.getPool().getChannelFoldingFactor() < 1
                    || layer.getPool().getChannelFoldingFactor() > layer.getNumInputs()) {
                raiseError(layer.getLayerId(),
                        "Channel folding factor in pooling layer must be strictly between "
                        + "inclusive 1 and number of input channels");
            }
            break;

        case LRN:
            if (layer.getLrn().getChannelFoldingFactor() < 1
                    || layer.getLrn().getChannelFoldingFactor() > layer.getNumInputs()) {
                raiseError(layer.getLayerId(),
                        "Channel folding factor in LRN layer must be strictly between "
                        + "inclusive 1 and number of input channels");
            }
            break;

        }
    }

    private int getAddressSeparation(LayerParameter layerParams) {
        int byteWidth = Utils.divCeil(
                layerParams.getConv().getKernelFoldingFactor()
                    * layerParams.getConv().getConvFoldingFactor()
                    * layerParams.getConv().getWorkerFactor()
                    * 4,
                384) * 384;
        IterationCounter ctr = new IterationCounter(layerParams);
        return byteWidth * ctr.getTotalIterations();
    }

    private boolean convInitFromCpu(LayerParameter layerParams) {
        IterationCounter ctr = new IterationCounter(layerParams);
        int fullOnChipBramFactor =
                layerParams.getConv().getWorkerFactor()
                * layerParams.getConv().getConvFoldingFactor()
                * ctr.getSchedulerIterations()
                * ctr.getConvolutionIterations();
        return fullOnChipBramFactor == layerParams.getConv().getBramFactor();
    }

    private LayerParameter allignIOWidth(
            LayerParameter prev,
            LayerParameter cur,
            LayerParameter next) {

        LayerParameter.Builder builder = LayerParameter.newBuilder();
        builder.mergeFrom(cur);
        builder.setOutputVectorWidth(cur.getNumOutputs());

        // TODO(fyq14): Figure out how to apply this to optimize FIFO usage
        //              for maxring. The difficulty lies in getting it right
        //              such that we don't have the input stream to be a
        //              multiple of a ridiculous number of multiples.

        if (next == null) {
            // allign width with PCIe (128 bits) and 32 bits output (at the
            // last layer, the numbers are casted to floating point).
            int size = 128 / 32;
            builder.setOutputVectorWidth(size);

        } else if (next != null && next.getFpgaId() != cur.getFpgaId()) {
            int size = 512 / 16;
            builder.setOutputVectorWidth(size);

        } else if (next.getParamsCase().equals(LayerParameter.ParamsCase.CONV)) {
            // allign width with next layer's worker factor
            builder.setOutputVectorWidth(next.getConv().getWorkerFactor());

        } else if (next.getParamsCase().equals(LayerParameter.ParamsCase.POOL)) {
            builder.setOutputVectorWidth(
                    next.getPool().getChannelFoldingFactor());

        }

        if (builder.getOutputVectorWidth() < cur.getNumOutputs()) {
            int runIterations = 0;

            if (cur.getParamsCase().equals(LayerParameter.ParamsCase.CONV)) {
                runIterations =
                    new IterationCounter(cur).getTotalIterations()
                    * cur.getConv().getStride();

            } else if (cur.getParamsCase().equals(LayerParameter.ParamsCase.POOL)) {
                runIterations =
                    Utils.divCeil(
                            cur.getNumOutputs(), cur.getPool().getChannelFoldingFactor())
                    * cur.getPool().getStride();

            } else if (cur.getParamsCase().equals(LayerParameter.ParamsCase.LRN)) {
                runIterations =
                    Utils.divCeil(
                            cur.getNumOutputs(), cur.getLrn().getChannelFoldingFactor());

            }
            int conversionCycles = -1;

            /*
             * find the largest divisor of numOutputs which is
             * <= output_vector_width
             */
            for (int i = builder.getOutputVectorWidth(); i >= 1 ; i--) {
                if (cur.getNumOutputs() % i == 0) {
                    conversionCycles = cur.getNumOutputs() / i;
                    break;
                }
            }
            if (conversionCycles == -1) {
                throw new RuntimeException("this shouldn't happen.");
            }

            /* This condition checks if it is difficult to efficiently pipeline the
             * size conversion. (It is very much possible tho - I am just lazy
             * to implement that for now as i don't see it happening really
             * often with large nets).
             */
            System.out.println(
                    "conversion cycles = " + conversionCycles
                    + ", runIterations = " + runIterations);
            if (conversionCycles >= runIterations) {
                builder.setOutputVectorWidth(cur.getNumOutputs());
            }
        }

        return builder.build();
    }

    public Parameters.Network transform() throws RuntimeException {
        Parameters.Network.Builder networkBuilder = Parameters.Network.newBuilder();
        int weightAddressBase = 0;
        int layerCount = networkParameters.getLayerCount();
        if (layerCount == 0) {
            raiseError(0, "Number of layers in networkParameters must be greater than 1");
        }

        ArrayList<LayerParameter> layersTemporal = new ArrayList<LayerParameter>();
        LayerParameter previousLayer = parseLayer(
                0, networkParameters.getLayer(0), null, weightAddressBase);
        verifyLayerParameters(previousLayer);
        layersTemporal.add(previousLayer);

        for (int i = 1; i < layerCount ; i++) {
            if (previousLayer.getParamsCase().equals(LayerParameter.ParamsCase.CONV)
                    && !convInitFromCpu(previousLayer)) {
                weightAddressBase += getAddressSeparation(previousLayer);
            }
            previousLayer = parseLayer(i, networkParameters.getLayer(i), previousLayer, weightAddressBase);
            verifyLayerParameters(previousLayer);
            layersTemporal.add(previousLayer);
        }

        for (int i = 0 ; i < layerCount ; i++) {
            LayerParameter cur = layersTemporal[i];
            LayerParameter prev = null;
            LayerParameter next = null;

            if (i != 0) {
                prev = layersTemporal[i - 1];
            }

            if (i != layerCount - 1) {
                next = layersTemporal[i + 1];
            }

            networkBuilder.addLayer(allignIOWidth(prev, cur, next));
        }

        networkBuilder.setFrequency(networkParameters.getFrequency());
        networkBuilder.setNumFpgaUsed(networkParameters.getNumFpgaUsed());
        return networkBuilder.build();
    }
}
