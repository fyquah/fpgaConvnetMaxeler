/* Most of the logic of resource alloc should go here. */

package fpgaconvnet;

import java.lang.RuntimeException;

import fpgaconvnet.protos.Parameters;
import fpgaconvnet.protos.Parameters.LayerParameter;
import fpgaconvnet.protos.Parameters.ConvolutionParameter;
import fpgaconvnet.protos.Parameters.PoolingParameter;


public class ParametersTransformer {
    private Parameters.Network networkParameters;

    public ParametersTransformer(Parameters.Network networkParametersIn) {
        networkParameters = networkParametersIn;
    }

    private void raiseError(int layer_id, String message) throws RuntimeException {
        throw new RuntimeException(String.format("Invalid layer spec at layer %d => %s",
                                         layer_id, message));
    }

    private void checkLayerDims() {

    }

    private LayerParameter parseLayer(int layer_id, LayerParameter layerParameter,
                                      LayerParameter previousLayer) {
        LayerParameter.Builder builder = LayerParameter.newBuilder();
        int inputHeight = 0;
        int inputWidth = 0;
        int numInputs = 0;
        int numOutputs = 0;
        builder.mergeFrom(layerParameter);

        /* Generic parameters (layer-type independent)
         * - inputHeight
         * - inputWidth
         * - numInputs
         * Layer-Dependent parameters
         * - outputHeight
         * - outputWidth
         * - numOutputs
         */
        if (layer_id == 0) {
            if (!layerParameter.hasInputHeight()
                    || !layerParameter.hasInputWidth()
                    || !layerParameter.hasNumInputs()) {
                raiseError(layer_id,
                           "First layer must specify the input height, width and num_inputs ("
                           + "i.e: The number of channels)");
            }
            inputHeight = layerParameter.getInputHeight();
            inputWidth = layerParameter.getInputWidth();
            numInputs = layerParameter.getNumInputs();
        } else {
            if (layer_id - 1 != previousLayer.getLayerId()) {
                raiseError(layer_id, "previousLayer.getLayerId() != layer_id - 1.");
            }
            if (layerParameter.hasInputHeight()
                    && previousLayer.getOutputHeight() != layerParameter.getInputHeight()) {
                raiseError(layer_id,
                           String.format("Input height mismatch - expecting %d but got %d instead",
                                         previousLayer.getOutputHeight(),
                                         layerParameter.getInputHeight()));
            }
            if (layerParameter.hasInputWidth()
                    && previousLayer.getOutputWidth() != layerParameter.getInputWidth()) {
                raiseError(layer_id,
                           String.format("Input Width mismatch - expecting %d but got %d instead",
                                         previousLayer.getOutputWidth(),
                                         layerParameter.getInputWidth()));
            }
            if (layerParameter.hasNumInputs()
                    && previousLayer.getNumOutputs() != layerParameter.getNumInputs()) {
                raiseError(layer_id,
                           String.format("Num Inputs mismatch - expecting %d but got %d instead",
                                         previousLayer.getNumOutputs(),
                                         layerParameter.getNumInputs()));
            }

            inputHeight = previousLayer.getOutputHeight();
            inputWidth = previousLayer.getOutputWidth();
            numInputs = previousLayer.getNumOutputs();
        }

        /* Calculate the new dimensions based on the data that we have on the ops. */
        LayerParameter.ParamsCase paramsCase = layerParameter.getParamsCase();
        int stride = -1;
        int pad = -1;
        int kernelSize = -1;

        if (paramsCase.equals(LayerParameter.ParamsCase.CONV)) {
            ConvolutionParameter convParams = layerParameter.getConv();

            if (!convParams.hasKernelSize() || convParams.getKernelSize() % 2 == 0) {
                raiseError(layer_id, ("Missing /invalid conv kernel_size. Conv's Kernel size "
                                      + "has to be an odd number."));
            }
            if (!layerParameter.hasNumOutputs() || layerParameter.getNumOutputs() == 0) {
                raiseError(layer_id, "numOutputs must be a positive integer in conv layers!");
            }

            stride = convParams.getStride();
            pad = convParams.getPad();
            stride = convParams.hasStride() ? convParams.getStride() : 1;
            kernelSize = convParams.getKernelSize();
            numOutputs = layerParameter.getNumOutputs();

            builder.setConv(
                    ConvolutionParameter.newBuilder()
                    .mergeFrom(convParams)
                    .setStride(stride)
                    .build());

        } else if (paramsCase.equals(LayerParameter.ParamsCase.POOL)) {
            PoolingParameter poolParams = layerParameter.getPool();

            if (layerParameter.hasNumOutputs() &&
                    layerParameter.getNumOutputs() != numInputs) {
                    raiseError(layer_id,
                               "num_inputs and num_outputs for pooling layer should be the same");
            }

            pad = 0;
            kernelSize = poolParams.getDim();
            stride = poolParams.hasStride() ? poolParams.getStride() : kernelSize;
            numOutputs = numInputs;

            builder.setPool(
                    PoolingParameter.newBuilder()
                    .mergeFrom(poolParams)
                    .setStride(stride)
                    .build());

        } else {
            raiseError(layer_id, "Either pool of conv has to be set!");

        }

        final int unstridedHeight = inputHeight - kernelSize + 2 * pad;
        final int unstridedWidth = inputWidth - kernelSize + 2 * pad;
        final int expectedHeight = unstridedHeight / stride + 1;
        final int expectedWidth = unstridedWidth / stride + 1;

        if (unstridedHeight % stride != 0) {
            raiseError(layer_id,
                       ("`stride` doesn't divide "
                        + "`input_height - kernel_size + 2 * pad`!"));
        }
        if (unstridedWidth % stride != 0) {
            raiseError(layer_id,
                       ("`stride` doesn't divide "
                        + "`input_width - kernel_size + 2 * pad`!"));
        }

        builder.setInputHeight(inputHeight);
        builder.setInputWidth(inputWidth);
        builder.setNumInputs(numInputs);
        builder.setOutputHeight(expectedHeight);
        builder.setOutputWidth(expectedWidth);
        builder.setNumOutputs(numOutputs);
        builder.setLayerId(layer_id);
        if (networkParameters.getLayerCount() - 1 == layer_id) {
            builder.setIsLastLayer(true);
        }
        if (layer_id == 0) {
            builder.setIsFirstLayer(true);
        }
        return builder.build();
    }

    public Parameters.Network transform() throws RuntimeException {
        Parameters.Network.Builder networkBuilder = Parameters.Network.newBuilder();
        int layerCount = networkParameters.getLayerCount();
        if (layerCount == 0) {
            raiseError(0, "Number of layers in networkParameters must be greater than 1");
        }
        LayerParameter previousLayer = parseLayer(0, networkParameters.getLayer(0), null);
        networkBuilder.addLayer(previousLayer);
        for (int i = 1; i < layerCount ; i++) {
            previousLayer = parseLayer(i, networkParameters.getLayer(i), previousLayer);
            networkBuilder.addLayer(previousLayer);
        }

        networkBuilder.setFrequency(networkParameters.getFrequency());
        return networkBuilder.build();
    }
}
