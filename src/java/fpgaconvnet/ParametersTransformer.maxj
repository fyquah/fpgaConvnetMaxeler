/* Most of the logic of resource alloc should go here. */

package fpgaconvnet;

import java.lang.RuntimeException;

import fpgaconvnet.protos.Parameters;
import fpgaconvnet.protos.Parameters.LayerParameter;
import fpgaconvnet.protos.Parameters.ConvolutionParameter;
import fpgaconvnet.protos.Parameters.PoolingParameter;
import fpgaconvnet.protos.Parameters.LrnParameter;


public class ParametersTransformer {
    private Parameters.Network networkParameters;

    public ParametersTransformer(Parameters.Network networkParametersIn) {
        networkParameters = networkParametersIn;
    }

    private void raiseError(int layer_id, String message) throws RuntimeException {
        throw new RuntimeException(String.format("Invalid layer spec at layer %d => %s",
                                         layer_id, message));
    }

    private void raiseWarning(int layer_id, String message) {
        System.out.println(String.format("[WARNING layer %d] => %s", layer_id, message));
    }

    private LayerParameter parseLayer(int layer_id, LayerParameter layerParameter,
                                      LayerParameter previousLayer) {
        LayerParameter.Builder builder = LayerParameter.newBuilder();
        int inputHeight = 0;
        int inputWidth = 0;
        int numInputs = 0;
        int numOutputs = 0;
        builder.mergeFrom(layerParameter);

        /* Generic parameters (layer-type independent)
         * - inputHeight
         * - inputWidth
         * - numInputs
         * Layer-Dependent parameters
         * - outputHeight
         * - outputWidth
         * - numOutputs
         */
        if (layer_id == 0) {
            if (!layerParameter.hasInputHeight()
                    || !layerParameter.hasInputWidth()
                    || !layerParameter.hasNumInputs()) {
                raiseError(layer_id,
                           "First layer must specify the input height, width and num_inputs ("
                           + "i.e: The number of channels)");
            }
            inputHeight = layerParameter.getInputHeight();
            inputWidth = layerParameter.getInputWidth();
            numInputs = layerParameter.getNumInputs();
        } else {
            if (layer_id - 1 != previousLayer.getLayerId()) {
                raiseError(layer_id, "previousLayer.getLayerId() != layer_id - 1.");
            }
            if (layerParameter.hasInputHeight()
                    && previousLayer.getOutputHeight() != layerParameter.getInputHeight()) {
                raiseError(layer_id,
                           String.format("Input height mismatch - expecting %d but got %d instead",
                                         previousLayer.getOutputHeight(),
                                         layerParameter.getInputHeight()));
            }
            if (layerParameter.hasInputWidth()
                    && previousLayer.getOutputWidth() != layerParameter.getInputWidth()) {
                raiseError(layer_id,
                           String.format("Input Width mismatch - expecting %d but got %d instead",
                                         previousLayer.getOutputWidth(),
                                         layerParameter.getInputWidth()));
            }
            if (layerParameter.hasNumInputs()
                    && previousLayer.getNumOutputs() != layerParameter.getNumInputs()) {
                raiseError(layer_id,
                           String.format("Num Inputs mismatch - expecting %d but got %d instead",
                                         previousLayer.getNumOutputs(),
                                         layerParameter.getNumInputs()));
            }

            inputHeight = previousLayer.getOutputHeight();
            inputWidth = previousLayer.getOutputWidth();
            numInputs = previousLayer.getNumOutputs();
        }

        /* Calculate the new dimensions based on the data that we have on the ops. */
        LayerParameter.ParamsCase paramsCase = layerParameter.getParamsCase();
        int stride = -1;
        int pad = -1;
        int kernelSize = -1;

        if (paramsCase.equals(LayerParameter.ParamsCase.CONV)) {
            ConvolutionParameter convParams = layerParameter.getConv();

            if (!convParams.hasKernelSize() || convParams.getKernelSize() % 2 == 0) {
                raiseError(layer_id, ("Missing /invalid conv kernel_size. Conv's Kernel size "
                                      + "has to be an odd number."));
            }
            if (!layerParameter.hasNumOutputs() || layerParameter.getNumOutputs() == 0) {
                raiseError(layer_id, "numOutputs must be a positive integer in conv layers!");
            }

            stride = convParams.getStride();
            pad = convParams.getPad();
            stride = convParams.hasStride() ? convParams.getStride() : 1;
            kernelSize = convParams.getKernelSize();
            numOutputs = layerParameter.getNumOutputs();

            builder.setConv(
                    ConvolutionParameter.newBuilder()
                    .mergeFrom(convParams)
                    .setStride(stride)
                    .build());

        } else if (paramsCase.equals(LayerParameter.ParamsCase.POOL)) {
            PoolingParameter poolParams = layerParameter.getPool();

            if (layerParameter.hasNumOutputs() &&
                    layerParameter.getNumOutputs() != numInputs) {
                    raiseError(layer_id,
                               "num_inputs and num_outputs for pooling layer should be the same");
            }

            pad = 0;
            kernelSize = poolParams.getDim();
            stride = poolParams.hasStride() ? poolParams.getStride() : kernelSize;
            numOutputs = numInputs;

            builder.setPool(
                    PoolingParameter.newBuilder()
                    .mergeFrom(poolParams)
                    .setStride(stride)
                    .build());

        } else if (paramsCase.equals(LayerParameter.ParamsCase.LRN)) {
            LrnParameter lrnParams = layerParameter.getLrn();

            numOutputs = numInputs;
            pad = 0;
            kernelSize = 1;
            stride = 1;

            if (layerParameter.hasNumOutputs() &&
                    layerParameter.getNumOutputs() != numInputs) {
                    raiseError(layer_id,
                               "num_inputs and num_outputs for LRN layer should be the same");
            }

            builder.setLrn(lrnParams);

        } else {
            raiseError(layer_id, "Either pool of conv has to be set!");

        }

        final int unstridedHeight = inputHeight - kernelSize + 2 * pad;
        final int unstridedWidth = inputWidth - kernelSize + 2 * pad;
        final int expectedHeight = Utils.divCeil(unstridedHeight, stride) + 1;
        final int expectedWidth = Utils.divCeil(unstridedWidth, stride) + 1;


        if (paramsCase.equals(LayerParameter.ParamsCase.CONV)) {
            if (unstridedHeight % stride != 0) {
                raiseError(layer_id,
                           ("conv layer `stride` must divide "
                            + "`input_height - kernel_size + 2 * pad`!"));
            }
            if (unstridedWidth % stride != 0) {
                raiseError(layer_id,
                           ("conv layer `stride` must divide "
                            + "`input_width - kernel_size + 2 * pad`!"));
            }

        } else {
            if (unstridedHeight % stride != 0) {
                raiseWarning(layer_id,
                             ("pool layer `stride` doesn't divide "
                              + "`input_height - kernel_size + 2 * pad`!"
                              + ". Border cases may be approximated"));
            }
            if (unstridedWidth % stride != 0) {
                raiseWarning(layer_id,
                             ("pool layer `stride` doesn't divide "
                              + "`input_width - kernel_size + 2 * pad`!"
                              + ". Border cases may be approximated!"));
            }

        }


        builder.setInputHeight(inputHeight);
        builder.setInputWidth(inputWidth);
        builder.setNumInputs(numInputs);
        builder.setOutputHeight(expectedHeight);
        builder.setOutputWidth(expectedWidth);
        builder.setNumOutputs(numOutputs);
        builder.setLayerId(layer_id);
        if (networkParameters.getLayerCount() - 1 == layer_id) {
            builder.setIsLastLayer(true);
        }
        if (layer_id == 0) {
            builder.setIsFirstLayer(true);
        }
        return builder.build();
    }

    private boolean checkOutOfRange(int x, int max) {
        return x < 1 || x > max;
    }

    public void verifyLayerParameters(LayerParameter layer) {
        LayerParameter.ParamsCase paramsCase = layer.getParamsCase();

        switch (paramsCase) {
        case CONV:
            int totalKernelSize = layer.getConv().getKernelSize() * layer.getConv().getKernelSize();

            if (checkOutOfRange(layer.getConv().getKernelFoldingFactor(), totalKernelSize)) {
                raiseError(layer.getLayerId(),
                        "kernel folding factor in convolution layer must be strictly between "
                        + "inclusive 1 and kernel_size * kernel_size");
            }
            if (checkOutOfRange(layer.getConv().getWorkerFactor(), layer.getNumInputs())) {
                raiseError(layer.getLayerId(),
                        "Worker factor in convolution layer must be strictly between "
                        + "inclusive 1 and num_inputs");
            }
            if (checkOutOfRange(layer.getConv().getConvFoldingFactor(), layer.getNumOutputs())) {
                raiseError(layer.getLayerId(),
                        "Conv folding factor in convolution layer must be strictly between "
                        + "inclusive 1 and num_outputs");
            }

            IterationCounter ctr = new IterationCounter(layer);
            if (layer.getConv().getBramFactor() % ctr.getConvolutionIterations() != 0) {
                raiseError(layer.getLayerId(),
                        "bram_factor in convolution layer must be a factor or a multiple of"
                        + " ceil(num_outputs / conv_folding_factor) unless it is equal to"
                        + " num_inputs * num_outputs");
            }

            int totalPixels = layer.getOutputHeight() * layer.getOutputWidth();
            if (layer.getConv().getLookAhead() < 1
                    || totalPixels % layer.getConv().getLookAhead() != 0
                    || layer.getConv().getLookAhead() > totalPixels) {
                raiseError(layer.getLayerId(),
                        "lookAhead must be > 1, a factor of total OUTPUT pixels and must be <="
                        + " the total number of pixels");
            }
            break;

        case POOL:
            if (layer.getPool().getChannelFoldingFactor() < 1
                    || layer.getPool().getChannelFoldingFactor() > layer.getNumInputs()) {
                raiseError(layer.getLayerId(),
                        "Channel folding factor in pooling layer must be strictly between "
                        + "inclusive 1 and number of input channels");
            }
            break;

        case LRN:
            if (layer.getLrn().getChannelFoldingFactor() < 1
                    || layer.getLrn().getChannelFoldingFactor() > layer.getNumInputs()) {
                raiseError(layer.getLayerId(),
                        "Channel folding factor in LRN layer must be strictly between "
                        + "inclusive 1 and number of input channels");
            }
            break;

        }
    }

    public Parameters.Network transform() throws RuntimeException {
        Parameters.Network.Builder networkBuilder = Parameters.Network.newBuilder();
        int layerCount = networkParameters.getLayerCount();
        if (layerCount == 0) {
            raiseError(0, "Number of layers in networkParameters must be greater than 1");
        }
        LayerParameter previousLayer = parseLayer(0, networkParameters.getLayer(0), null);
        networkBuilder.addLayer(previousLayer);
        for (int i = 1; i < layerCount ; i++) {
            previousLayer = parseLayer(i, networkParameters.getLayer(i), previousLayer);
            verifyLayerParameters(previousLayer);
            networkBuilder.addLayer(previousLayer);
        }

        networkBuilder.setFrequency(networkParameters.getFrequency());
        return networkBuilder.build();
    }
}
