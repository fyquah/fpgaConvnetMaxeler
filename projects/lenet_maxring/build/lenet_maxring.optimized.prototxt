layer {
  conv {
    worker_factor: 1
    conv_folding_factor: 20
    kernel_folding_factor: 25
    bram_factor: 20
    look_ahead: 1
    kernel_size: 5
    should_fit_on_chip: true
  }
  input_height: 28
  input_width: 28
  num_inputs: 1
  num_outputs: 20
  activation: Relu
  output_height: 24
  output_width: 24
  layer_id: 0
  is_first_layer: true
  fpga_id: 0
}
layer {
  pool {
    type: Max
    dim: 2
    stride: 2
    channel_folding_factor: 20
  }
  input_height: 24
  input_width: 24
  num_inputs: 20
  num_outputs: 20
  output_height: 12
  output_width: 12
  layer_id: 1
  fpga_id: 0
}
layer {
  conv {
    worker_factor: 10
    conv_folding_factor: 13
    kernel_folding_factor: 13
    bram_factor: 1040
    # worker_factor: 3
    # conv_folding_factor: 50
    # kernel_folding_factor: 13
    # bram_factor: 1050
    look_ahead: 1
    kernel_size: 5
    should_fit_on_chip: true
  }
  input_height: 12
  input_width: 12
  num_inputs: 20
  num_outputs: 50
  activation: Relu
  output_height: 8
  output_width: 8
  layer_id: 2
  fpga_id: 1
}
layer {
  pool {
    type: Max
    dim: 2
    stride: 2
    # channel_folding_factor: 5
    channel_folding_factor: 17
  }
  input_height: 8
  input_width: 8
  num_inputs: 50
  num_outputs: 50
  output_height: 4
  output_width: 4
  layer_id: 3
  is_last_layer: true
  fpga_id: 1
}
num_fpga_available: 2
num_fpga_used: 2
