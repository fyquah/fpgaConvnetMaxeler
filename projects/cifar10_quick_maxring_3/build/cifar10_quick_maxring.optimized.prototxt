layer {
  conv {
    worker_factor: 3
    conv_folding_factor: 16
    kernel_folding_factor: 25
    bram_factor: 96
    look_ahead: 1
    kernel_size: 5
    stride: 1
    pad: 2
    should_fit_on_chip: true
    weight_address_base: 0
  }
  input_height: 32
  input_width: 32
  num_inputs: 3
  num_outputs: 32
  output_height: 32
  output_width: 32
  layer_id: 0
  is_first_layer: true
  fpga_id: 0
}
layer {
  pool {
    type: Max
    dim: 3
    stride: 2
    channel_folding_factor: 16
  }
  input_height: 32
  input_width: 32
  num_inputs: 32
  num_outputs: 32
  activation: Relu
  output_height: 16
  output_width: 16
  layer_id: 1
  fpga_id: 0
}
layer {
  conv {
    worker_factor: 11
    conv_folding_factor: 7
    kernel_folding_factor: 25
    bram_factor: 1155
    look_ahead: 1
    kernel_size: 5
    stride: 1
    pad: 2
    should_fit_on_chip: true
    weight_address_base: 0
  }
  input_height: 16
  input_width: 16
  num_inputs: 32
  num_outputs: 32
  activation: Relu
  output_height: 16
  output_width: 16
  layer_id: 2
  fpga_id: 1
}
layer {
  pool {
    type: Average
    dim: 3
    stride: 2
    channel_folding_factor: 32
  }
  input_height: 16
  input_width: 16
  num_inputs: 32
  num_outputs: 32
  output_height: 8
  output_width: 8
  layer_id: 3
  fpga_id: 2
}
layer {
  conv {
    worker_factor: 32
    conv_folding_factor: 8
    kernel_folding_factor: 5
    bram_factor: 2048
    look_ahead: 1
    kernel_size: 5
    stride: 1
    pad: 2
    should_fit_on_chip: true
    weight_address_base: 0
  }
  input_height: 8
  input_width: 8
  num_inputs: 32
  num_outputs: 64
  activation: Relu
  output_height: 8
  output_width: 8
  layer_id: 4
  fpga_id: 2
}
layer {
  pool {
    type: Average
    dim: 3
    stride: 2
    channel_folding_factor: 5
  }
  input_height: 8
  input_width: 8
  num_inputs: 64
  num_outputs: 64
  output_height: 4
  output_width: 4
  layer_id: 5
  is_last_layer: true
  fpga_id: 2
}
num_fpga_available: 3
num_fpga_used: 3
