layer {
  conv {
    worker_factor: 3
    conv_folding_factor: 16
    kernel_folding_factor: 25
    kernel_size: 5
    pad: 2
  }
  input_height: 32
  input_width: 32
  num_inputs: 3
  num_outputs: 32
  output_height: 32
  output_width: 32
  layer_id: 0
  fpga_id: 0
}
layer {
  pool {
    type: Max
    dim: 3
    stride: 2
    channel_folding_factor: 16
  }
  input_height: 32
  input_width: 32
  num_inputs: 32
  num_outputs: 32
  activation: Relu
  output_height: 16
  output_width: 16
  layer_id: 1
  fpga_id: 0
}
layer {
  conv {
    worker_factor: 4
    conv_folding_factor: 32
    kernel_folding_factor: 25
    kernel_size: 5
    stride: 1
    pad: 2
  }
  input_height: 16
  input_width: 16
  num_inputs: 32
  num_outputs: 32
  activation: Relu
  output_height: 16
  output_width: 16
  layer_id: 2
  fpga_id: 1
}
layer {
  pool {
    type: Average
    dim: 3
    stride: 2
    channel_folding_factor: 4
  }
  input_height: 16
  input_width: 16
  num_inputs: 32
  num_outputs: 32
  output_height: 8
  output_width: 8
  layer_id: 3
  fpga_id: 1
}
layer {
  conv {
    worker_factor: 1
    conv_folding_factor: 64
    kernel_folding_factor: 25
    kernel_size: 5
    stride: 1
    pad: 2
  }
  input_height: 8
  input_width: 8
  num_inputs: 32
  num_outputs: 64
  activation: Relu
  output_height: 8
  output_width: 8
  layer_id: 4
  fpga_id: 2
}
layer {
  pool {
    type: Average
    dim: 3
    stride: 2
    channel_folding_factor: 2
  }
  input_height: 8
  input_width: 8
  num_inputs: 64
  num_outputs: 64
  output_height: 4
  output_width: 4
  layer_id: 5
  fpga_id: 2
}
num_fpga_available: 3
num_fpga_used: 3
